{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63a52471",
   "metadata": {},
   "source": [
    "# end 2 end model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5381a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class HParams:\n",
    "    def __init__(self):\n",
    "        self.num_measurements = 1500\n",
    "        self.layer_sizes = [50, 200]\n",
    "        self.train_batch_size = 8\n",
    "        self.test_batch_size = 10\n",
    "        self.learning_rate = 0.001\n",
    "        self.max_train_steps = 15000   # 50000\n",
    "        self.summary_iter = 1000\n",
    "        self.checkpoint_iter = 20000\n",
    "        self.is_A_trainable = False\n",
    "        self.noise_std = 0.1\n",
    "        self.dataset = \"mmnist\"           # mmnist or ucf\n",
    "\n",
    "hparams = HParams()\n",
    "\n",
    "hparams.dataset = \"mmnist\"\n",
    "hparams.num_measurements = 300   # for different measurements, train new model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c18b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving MNIST model \n",
    "class E2EAutoencoder(nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.input_dim = 64 * 64  \n",
    "        self.A = nn.Linear(self.input_dim, hparams.num_measurements, bias=False)\n",
    "        self.A.weight.data.normal_(std=1.0/hparams.num_measurements)\n",
    "        self.A.weight.requires_grad = hparams.is_A_trainable\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = hparams.num_measurements\n",
    "        for size in hparams.layer_sizes:\n",
    "            layers.append(nn.Linear(prev_size, size))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = size\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(prev_size, self.input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.noise_std = hparams.noise_std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_flat = x.view(x.size(0),1, -1)  # [B*T, 64*64]\n",
    "        y = self.A(x_flat)              \n",
    "    \n",
    "        if not self.training:  # add noise only test\n",
    "            noise = torch.normal(mean=0, std=self.noise_std, size=y.size(), device=y.device)\n",
    "            y += noise\n",
    "        hidden = self.encoder(y)\n",
    "        recon = self.decoder(hidden)    \n",
    "        return recon.view_as(x)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc5b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UCF model \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class E2EAutoencoderUCF(nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.input_dim = 3 * 64 * 64  \n",
    "        self.A = nn.Linear(self.input_dim, hparams.num_measurements, bias=False)\n",
    "        \n",
    "        self.A.weight.data = torch.randn_like(self.A.weight.data)\n",
    "        self.A.weight.requires_grad = hparams.is_A_trainable\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = hparams.num_measurements\n",
    "        for size in hparams.layer_sizes:\n",
    "            layers.append(nn.Linear(prev_size, size))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = size\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(prev_size, self.input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.noise_std = hparams.noise_std\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x_flat = x.view(x.size(0), -1)  # [B, 3*64*64]\n",
    "        y = self.A(x_flat)     \n",
    "        \n",
    "        if not self.training:  \n",
    "            noise = torch.normal(mean=0, std=self.noise_std, size=y.size(), device=y.device)\n",
    "            y += noise\n",
    "        hidden = self.encoder(y)\n",
    "        recon = self.decoder(hidden)    \n",
    "        return recon.view_as(x)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235824b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Moving Mnist data\n",
    "import torch\n",
    "from MovingMNISTVideoLoader import MovingMNIST\n",
    "\n",
    "\n",
    "train_set = MovingMNIST(root='data/Movingmnist', start=0, end=10, train=True, download=True)\n",
    "test_set = MovingMNIST(root='data/Movingmnist', start=0, end=10, train=False, download=True)\n",
    "\n",
    "\n",
    "train_loader_mmnist = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=hparams.train_batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader_mmnist = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=hparams.test_batch_size,\n",
    "                shuffle=True)\n",
    "\n",
    "print('==>>> total trainning batch number: {}'.format(len(train_loader_mmnist)))\n",
    "print('==>>> total testing batch number: {}'.format(len(test_loader_mmnist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277d7b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    " # load ucf\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from src.UCF_handler import UCF101VideoDataset\n",
    "dataset = UCF101VideoDataset(video_folder=\"VideoGeneration-PyTorch-main/data/UCF101\", transform=None, start=0, end=10)\n",
    "\n",
    "train_size = int(0.9 * len(dataset)) \n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# DataLoader\n",
    "train_loader_ucf = DataLoader(train_dataset, batch_size=hparams.train_batch_size, shuffle=True)\n",
    "test_loader_ucf = DataLoader(test_dataset, batch_size=hparams.test_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37013e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hparams.dataset == \"mmnist\":\n",
    "    model = E2EAutoencoder(hparams).to(device) \n",
    "    train_loader, test_loader = train_loader_mmnist, test_loader_mmnist\n",
    "else:\n",
    "    model = E2EAutoencoderUCF(hparams).to(device) \n",
    "    train_loader, test_loader = train_loader_ucf, test_loader_ucf\n",
    "    \n",
    "criterion = nn.BCELoss()  # sigmoid\n",
    "optimizer = optim.Adam(model.parameters(), lr=hparams.learning_rate)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    for step in range(hparams.max_train_steps):\n",
    "        #  [B, T, C, H, W] -> [B*T, C, H, W]  \n",
    "        if hparams.dataset==\"mmnist\":\n",
    "            data, _ = next(iter(train_loader))\n",
    "            data = data.view(-1, 1, 64, 64).to(device)  # [B*T, 1, 64, 64]\n",
    "        else:\n",
    "            data= next(iter(train_loader))\n",
    "            data = data.view(-1, 3, 64, 64).to(device)  # [B*T, 1, 64, 64]\n",
    "        \n",
    "        # [-1, 1]-> [0, 1]\n",
    "        data = (data + 1) / 2\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        recon = model(data)\n",
    "        loss = criterion(recon, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % hparams.summary_iter == 0:\n",
    "            print(f\"Step {step}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "    # save\n",
    "    torch.save(model.state_dict(), f'e2e_movingmnist_m300model.pth')\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03890a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# test\n",
    "model = E2EAutoencoderUCF(hparams).to(device)\n",
    "model.load_state_dict(torch.load(\"e2e_ucf_m2000model.pth\", map_location=device))\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data = next(iter(test_loader))\n",
    "    \n",
    "        data = (data + 1) / 2  \n",
    "        print(data.shape)\n",
    "        data = data.view(-1, 3, 64, 64)  \n",
    "        data = data.to(device)\n",
    "        print(data.shape)\n",
    "\n",
    "        recon = model(data)\n",
    "        test_loss = criterion(recon, data)\n",
    "        print(f\"Test Loss: {test_loss.item():.4f}\")\n",
    "\n",
    "        fig, axes = plt.subplots(2, 10, figsize=(20, 4))\n",
    "\n",
    "        for i in range(min(30, 10*hparams.test_batch_size)):  \n",
    "            axes[0, i].imshow(data[i].cpu().detach().numpy().transpose(1, 2, 0))  \n",
    "            axes[0, i].axis(\"off\")\n",
    "            axes[1, i].imshow(recon[i].cpu().detach().numpy().transpose(1, 2, 0)) \n",
    "            axes[1, i].axis(\"off\")\n",
    "        plt.show()\n",
    "        \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b802a13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
